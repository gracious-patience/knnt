{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing, make_friedman1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import dnnr\n",
    "import catboost\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression datasets with only numerical features from https://arxiv.org/pdf/2207.08815.pdf \n",
    "# (Why do tree-based models still outperform deep learning on tabular data?)\n",
    "links = [\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103266/dataset\", \"name\": \"Brazilian_houses\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103261/dataset\", \"name\":\"wine\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103262/dataset\", \"name\": \"ailerons\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103263/dataset\", \"name\": \"houses\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103264/dataset\", \"name\": \"house_16H\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103267/dataset\", \"name\": \"Bike_Sharing_Demand\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103268/dataset\", \"name\": \"nyc-taxi-green-dec-2016\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103269/dataset\", \"name\": \"house_sales\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103270/dataset\", \"name\": \"sulfur\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103271/dataset\", \"name\": \"medical_charges\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103272/dataset\", \"name\": \"MiamiHousing2016\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103273/dataset\", \"name\": \"superconduct\"},\n",
    "    {\"url\": \"\", \"name\": \"cpu\"},\n",
    "    {\"url\": \"\", \"name\": \"diamond\"},\n",
    "    {\"url\": \"\", \"name\": \"isolet\"},\n",
    "    {\"url\": \"\", \"name\": \"pol\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom datasets\n",
    "cpu = pd.read_csv(\"datasets/houses\")\n",
    "features = list(cpu.columns)[:-1]\n",
    "labels = list(cpu.columns)[-1]\n",
    "cpu_X = cpu[cpu.columns.intersection(features)]\n",
    "cpu_y = cpu[cpu.columns.intersection([labels])]\n",
    "X, y = cpu_X.to_numpy(), cpu_y.to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit learn datasets\n",
    "X, y = fetch_california_housing( return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn_sc = dnnr.DNNR(n_neighbors=3, n_derivative_neighbors=-1, scaling=\"learned\")\n",
    "dn_scaling = dn_sc._get_scaler()\n",
    "standard_scaling = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2022)\n",
    "# X_train = standard_scaling.fit_transform(X_train)\n",
    "# X_test = standard_scaling.transform(X_test)\n",
    "\n",
    "# X_dn_scaled_train = dn_scaling.fit_transform(X_train, y_train)\n",
    "# X_dn_scaled_test = dn_scaling.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39457480771273146\n",
      "0.42847062197283137\n",
      "0.30678436347206606\n",
      "0.34693216951626576\n"
     ]
    }
   ],
   "source": [
    "neigh_r.fit(X_train, y_train)\n",
    "print(mean_squared_error(y_test, neigh_r.predict(X_test)))\n",
    "\n",
    "dn.fit(X_train, y_train)\n",
    "print(mean_squared_error(y_test, dn.predict(X_test)))\n",
    "\n",
    "neigh_r.fit(X_dn_scaled_train, y_train)\n",
    "print(mean_squared_error(y_test, neigh_r.predict(X_dn_scaled_test)))\n",
    "\n",
    "# dn = dnnr.DNNR(n_neighbors=k,n_derivative_neighbors=k_s, scaling=None, solver=\"linear_regression\")\n",
    "dn = dnnr.DNNR(n_neighbors=k,n_derivative_neighbors=k_s, scaling=None, solver=\"scipy_lsqr\")\n",
    "dn.fit(X_dn_scaled_train, y_train)\n",
    "print(mean_squared_error(y_test, dn.predict(X_dn_scaled_test)))\n",
    "\n",
    "# dn_sc = dnnr.DNNR(n_neighbors=k, n_derivative_neighbors=k_s, scaling=\"learned\", solver=\"scipy_lsqr\")\n",
    "# dn_sc.fit(X_train, y_train)\n",
    "# print(mean_squared_error(y_test, dn_sc.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5109854086585075, 0.20642617904547333)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kNN no learned scaling\n",
    "neigh_r = KNeighborsRegressor(n_neighbors=5)\n",
    "reg = make_pipeline(standard_scaling, neigh_r)\n",
    "scores = cross_val_score(reg, X, y, scoring='r2', cv=10, n_jobs=8)\n",
    "scores.mean(), scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7809106676064816, 0.010070034496825595)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kNN with learned scaling\n",
    "neigh_r = KNeighborsRegressor(n_neighbors=5)\n",
    "reg = make_pipeline(standard_scaling, dn_scaling, neigh_r)\n",
    "scores = cross_val_score(reg, X_train, y_train, scoring='r2', cv=10, n_jobs=8)\n",
    "scores.mean(), scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7365074087090699, 0.02171473832580501)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# knnt no learned scaling\n",
    "dn_sc = dnnr.DNNR(n_neighbors=5, n_derivative_neighbors=32, scaling=None, solver=\"scipy_lsqr\", order=\"1\")\n",
    "reg = make_pipeline(standard_scaling, dn_sc)\n",
    "scores = cross_val_score(reg, X_train, y_train, scoring='r2', cv=10, n_jobs=1)\n",
    "scores.mean(), scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8081242083660628, 0.011242656235956244)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# knnt with learned scaling\n",
    "dn_sc = dnnr.DNNR(n_neighbors=10, n_derivative_neighbors=64, scaling=\"learned\", solver=\"scipy_lsqr\", order=\"1\")\n",
    "reg = make_pipeline(standard_scaling, dn_sc)\n",
    "scores = cross_val_score(reg, X_train, y_train, scoring='r2', cv=10, n_jobs=1)\n",
    "scores.mean(), scores.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перебор гиперпараметров для каждого датасе с записью в виде csv-файлов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with pol\n",
      "pol,2,uniform,0,r2,0.9298270754808513,0.00801976044102319\n",
      "pol,2,uniform,1,r2,0.8843016624425836,0.05528711992750621\n",
      "pol,2,distance,0,r2,0.931950395518407,0.007928304530536822\n",
      "pol,2,distance,1,r2,0.8874577967852666,0.05422164702566613\n",
      "pol,5,uniform,0,r2,0.9318412898260592,0.0059983803852474244\n",
      "pol,5,uniform,1,r2,0.8865350385639819,0.05217492619127895\n",
      "pol,5,distance,0,r2,0.9366703767094255,0.005847217678346791\n",
      "pol,5,distance,1,r2,0.8938303190357605,0.049691523885921886\n",
      "pol,7,uniform,0,r2,0.930084365077208,0.006102969552374869\n",
      "pol,7,uniform,1,r2,0.8809791919407319,0.05541506913959745\n",
      "pol,7,distance,0,r2,0.935693284954738,0.0059067021331935515\n",
      "pol,7,distance,1,r2,0.8904634873626446,0.05168675695401286\n",
      "pol,10,uniform,0,r2,0.9264364203133428,0.0068278003210306805\n",
      "pol,10,uniform,1,r2,0.8738440532854617,0.059610110191960664\n",
      "pol,10,distance,0,r2,0.9329664783990225,0.006544592534932005\n",
      "pol,10,distance,1,r2,0.8851620576655342,0.054737606039159156\n",
      "pol,25,uniform,0,r2,0.9090456779239784,0.008907621136836557\n",
      "pol,25,uniform,1,r2,0.8405679298570352,0.07366672427148621\n",
      "pol,25,distance,0,r2,0.9186904756663816,0.008255978764091305\n",
      "pol,25,distance,1,r2,0.8588697007610913,0.06498239299532145\n",
      "pol,50,uniform,0,r2,0.8847792216724489,0.01107596768813127\n",
      "pol,50,uniform,1,r2,0.803197463257046,0.08804430355015556\n",
      "pol,50,distance,0,r2,0.8989051177829361,0.009883133560159067\n",
      "pol,50,distance,1,r2,0.8277643203855988,0.07693388678694922\n",
      "pol,100,uniform,0,r2,0.8355561443820261,0.018034452316306484\n",
      "pol,100,uniform,1,r2,0.7476286718868954,0.098874381154432\n",
      "pol,100,distance,0,r2,0.8607625340140859,0.015131926603829145\n",
      "pol,100,distance,1,r2,0.782245809873624,0.0874267632108438\n",
      "pol,250,uniform,0,r2,0.6826478130767641,0.02978889363087963\n",
      "pol,250,uniform,1,r2,0.6110989350725914,0.08787134472392899\n",
      "pol,250,distance,0,r2,0.7461907551136056,0.024578057907184144\n",
      "pol,250,distance,1,r2,0.6748634997452696,0.08375593764188165\n"
     ]
    }
   ],
   "source": [
    "# kNN\n",
    "# Regression datasets with only numerical features from https://arxiv.org/pdf/2207.08815.pdf \n",
    "# (Why do tree-based models still outperform deep learning on tabular data?)\n",
    "links = [\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103266/dataset\", \"name\": \"Brazilian_houses\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103261/dataset\", \"name\":\"wine\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103262/dataset\", \"name\": \"ailerons\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103263/dataset\", \"name\": \"houses\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103264/dataset\", \"name\": \"house_16H\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103267/dataset\", \"name\": \"Bike_Sharing_Demand\"},\n",
    "    # {\"url\": \"https://api.openml.org/data/download/22103268/dataset\", \"name\": \"nyc-taxi-green-dec-2016\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103269/dataset\", \"name\": \"house_sales\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103270/dataset\", \"name\": \"sulfur\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103271/dataset\", \"name\": \"medical_charges\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103272/dataset\", \"name\": \"MiamiHousing2016\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103273/dataset\", \"name\": \"superconduct\"},\n",
    "    {\"url\": \"\", \"name\": \"cpu\"},\n",
    "    {\"url\": \"\", \"name\": \"diamond\"},\n",
    "    # {\"url\": \"\", \"name\": \"isolet\"},\n",
    "    {\"url\": \"\", \"name\": \"pol\"},\n",
    "]\n",
    "\n",
    "with open(\"./results/knn.csv\", \"a+\") as f:\n",
    "    # first line in csv file\n",
    "    f.write(\"dataset,k,weights,scaling,metric,mean,std\\n\")\n",
    "    # iterate over datasets\n",
    "    for dataset in links[:]:\n",
    "        # read data\n",
    "        data = pd.read_csv(f\"./datasets/{dataset['name']}\")\n",
    "        print(f\"working with {dataset['name']}\")\n",
    "        features = list(data.columns)[:-1]\n",
    "        labels = list(data.columns)[-1]\n",
    "        data_X = data[data.columns.intersection(features)]\n",
    "        data_y = data[data.columns.intersection([labels])]\n",
    "        X, y = data_X.to_numpy(), data_y.to_numpy().flatten()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2024)\n",
    "        # as in DNNR paper vary hyperparameters according to the size of dataset\n",
    "        # small\n",
    "        if X.shape[0] < 2000:\n",
    "            ks = [2,5,7,10,20,30,40,50]\n",
    "        # medium\n",
    "        elif X.shape[0] < 50000:\n",
    "            ks = [2,5,7,10,25,50,100,250]\n",
    "        # large\n",
    "        else:\n",
    "            ks = [2,3,5,7,10,12,15,20,25]\n",
    "\n",
    "        for k in ks:\n",
    "            for weights in [\"uniform\",\"distance\"]:\n",
    "                for scaling in [0, 1]:\n",
    "                    for metric in [\"neg_mean_squared_error\"]:\n",
    "                        model = KNeighborsRegressor(n_neighbors=k, weights=weights)\n",
    "                        if scaling:\n",
    "                            reg = make_pipeline(standard_scaling, dn_scaling, model)\n",
    "                            scores = cross_val_score(reg, X_train, y_train, scoring=metric, cv=10, n_jobs=2)  \n",
    "                        else:\n",
    "                            reg = make_pipeline(standard_scaling, model)\n",
    "                            scores = cross_val_score(reg, X_train, y_train, scoring=metric, cv=10, n_jobs=2)\n",
    "                        mean, std = -scores.mean(), scores.std()\n",
    "                        print(f\"{dataset['name']},{k},{weights},{scaling},{metric},{mean},{std}\")\n",
    "                        f.write(f\"{dataset['name']},{k},{weights},{scaling},{metric},{mean},{std}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with pol\n",
      "pol,0.001,3,50,r2,0.025564772724568384,0.0014856799853414887\n",
      "pol,0.001,3,50,mse,1694.2548918133737,47.88480514162721\n",
      "pol,0.001,3,100,r2,0.050026524259107544,0.0014565977465054575\n",
      "pol,0.001,3,100,mse,1651.7189781787524,46.52946819619643\n",
      "pol,0.001,3,500,r2,0.18742104106447324,0.002897194968343337\n",
      "pol,0.001,3,500,mse,1412.7615386502293,37.50303677690118\n",
      "pol,0.001,3,1000,r2,0.27594991315599005,0.0049924589717524155\n",
      "pol,0.001,3,1000,mse,1258.732455125133,29.862637971547265\n",
      "pol,0.001,5,50,r2,0.03418112064608551,0.001468130575626802\n",
      "pol,0.001,5,50,mse,1679.2774744257956,47.59417813408519\n",
      "pol,0.001,5,100,r2,0.0662791943346033,0.0015477821774392228\n",
      "pol,0.001,5,100,mse,1623.4671309280754,45.97851588588881\n",
      "pol,0.001,5,500,r2,0.24876362569418617,0.0032738599110238354\n",
      "pol,0.001,5,500,mse,1306.1156205742984,35.00852369313639\n",
      "pol,0.001,5,1000,r2,0.3718246753887972,0.005542761252014894\n",
      "pol,0.001,5,1000,mse,1092.0196691496071,24.97833357354331\n",
      "pol,0.001,10,50,r2,0.05246257245729143,0.0018068492537637996\n",
      "pol,0.001,10,50,mse,1647.4936136338704,46.83319657917572\n",
      "pol,0.001,10,100,r2,0.10192278645476438,0.0019808530241178525\n",
      "pol,0.001,10,100,mse,1561.489194055267,44.15093574294325\n",
      "pol,0.001,10,500,r2,0.38873289012843354,0.00516326746876821\n",
      "pol,0.001,10,500,mse,1062.7333956467478,28.49667388885263\n",
      "pol,0.001,10,1000,r2,0.5820350547063662,0.007004350123617935\n",
      "pol,0.001,10,1000,mse,726.5360451661866,17.49233815201989\n",
      "pol,0.01,3,50,r2,0.1878907629571228,0.0028866243902165387\n",
      "pol,0.01,3,50,mse,1411.9507519506403,37.68943781779569\n",
      "pol,0.01,3,100,r2,0.2775379829366101,0.0051016331740871045\n",
      "pol,0.01,3,100,mse,1255.9714386569722,29.843547584683684\n",
      "pol,0.01,3,500,r2,0.5131132186702281,0.00995822922514556\n",
      "pol,0.01,3,500,mse,846.1413976747066,13.207523567334412\n",
      "pol,0.01,3,1000,r2,0.639143920363191,0.010281002376530684\n",
      "pol,0.01,3,1000,mse,626.9901596628958,9.428988666508316\n",
      "pol,0.01,5,50,r2,0.2482306578991893,0.003260746258757059\n",
      "pol,0.01,5,50,mse,1307.0182258246657,34.124564743125504\n",
      "pol,0.01,5,100,r2,0.3716517517038361,0.005534858118780122\n",
      "pol,0.01,5,100,mse,1092.3107110120557,24.537241140872116\n",
      "pol,0.01,5,500,r2,0.6828481243769389,0.009328222604815282\n",
      "pol,0.01,5,500,mse,551.0665550700844,9.972148979446459\n",
      "pol,0.01,5,1000,r2,0.7896572575786864,0.009397155903920427\n",
      "pol,0.01,5,1000,mse,365.3583923285096,10.196345689808657\n",
      "pol,0.01,10,50,r2,0.3895974243850252,0.004992511660228595\n",
      "pol,0.01,10,50,mse,1061.2213019333678,28.0248678575139\n",
      "pol,0.01,10,100,r2,0.5847703605015814,0.006954220723620553\n",
      "pol,0.01,10,100,mse,721.8067329925088,18.39128899961939\n",
      "pol,0.01,10,500,r2,0.8738039742039121,0.007264455252287219\n",
      "pol,0.01,10,500,mse,219.13835301062454,8.54912961181974\n",
      "pol,0.01,10,1000,r2,0.9100022593084482,0.005820461311812007\n",
      "pol,0.01,10,1000,mse,156.23127025106405,6.574693394839566\n",
      "pol,0.1,3,50,r2,0.5223728414302264,0.01302876685102529\n",
      "pol,0.1,3,50,mse,829.9609991775471,15.725196299553224\n",
      "pol,0.1,3,100,r2,0.6439256589509386,0.011087254481831349\n",
      "pol,0.1,3,100,mse,618.6377912116466,9.593055030176032\n",
      "pol,0.1,3,500,r2,0.7998931463111363,0.01063732402017783\n",
      "pol,0.1,3,500,mse,347.5195870471315,12.317547110958536\n",
      "pol,0.1,3,1000,r2,0.827958056562119,0.00939487211273293\n",
      "pol,0.1,3,1000,mse,298.7206564989293,9.523556652835882\n",
      "pol,0.1,5,50,r2,0.688192364477773,0.010110027694221724\n",
      "pol,0.1,5,50,mse,541.7674288337836,11.709346682542066\n",
      "pol,0.1,5,100,r2,0.7886271031445542,0.00849170713540286\n",
      "pol,0.1,5,100,mse,367.240526861895,11.162517203561855\n",
      "pol,0.1,5,500,r2,0.8828027743790583,0.006598023931966327\n",
      "pol,0.1,5,500,mse,203.49469888944836,7.310198749473431\n"
     ]
    }
   ],
   "source": [
    "# catboost\n",
    "\n",
    "# Regression datasets with only numerical features from https://arxiv.org/pdf/2207.08815.pdf \n",
    "# (Why do tree-based models still outperform deep learning on tabular data?)\n",
    "links = [\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103266/dataset\", \"name\": \"Brazilian_houses\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103261/dataset\", \"name\":\"wine\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103262/dataset\", \"name\": \"ailerons\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103263/dataset\", \"name\": \"houses\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103264/dataset\", \"name\": \"house_16H\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103267/dataset\", \"name\": \"Bike_Sharing_Demand\"},\n",
    "    # {\"url\": \"https://api.openml.org/data/download/22103268/dataset\", \"name\": \"nyc-taxi-green-dec-2016\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103269/dataset\", \"name\": \"house_sales\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103270/dataset\", \"name\": \"sulfur\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103271/dataset\", \"name\": \"medical_charges\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103272/dataset\", \"name\": \"MiamiHousing2016\"},\n",
    "    {\"url\": \"https://api.openml.org/data/download/22103273/dataset\", \"name\": \"superconduct\"},\n",
    "    {\"url\": \"\", \"name\": \"cpu\"},\n",
    "    {\"url\": \"\", \"name\": \"diamond\"},\n",
    "    {\"url\": \"\", \"name\": \"isolet\"},\n",
    "    {\"url\": \"\", \"name\": \"pol\"},\n",
    "]\n",
    "\n",
    "# preprocessing\n",
    "standard_scaling = StandardScaler()\n",
    "\n",
    "# kNNt\n",
    "with open(\"./results/catboost.csv\", \"a+\") as f:\n",
    "    # first line in csv file\n",
    "    f.write(\"dataset,lr,max_depth,n_estimators,metric,mean,std\\n\")\n",
    "    # iterate over datasets\n",
    "    for dataset in links[-1:]:\n",
    "        # read data\n",
    "        data = pd.read_csv(f\"./datasets/{dataset['name']}\")\n",
    "        print(f\"working with {dataset['name']}\")\n",
    "        features = list(data.columns)[:-1]\n",
    "        labels = list(data.columns)[-1]\n",
    "        data_X = data[data.columns.intersection(features)]\n",
    "        data_y = data[data.columns.intersection([labels])]\n",
    "        X, y = data_X.to_numpy(), data_y.to_numpy().flatten()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2024)\n",
    "        # as in DNNR paper vary hyperparameters according to the size of dataset\n",
    "\n",
    "        for lr in [0.001,0.01,0.1,0.3]:\n",
    "            for max_depth in [3,5,10]:\n",
    "                for n_estimators in [50,100,500,1000]:\n",
    "                    # for metric in [\"r2\"]:\n",
    "                    model = catboost.CatBoostRegressor(verbose=False, learning_rate=lr, max_depth=max_depth, n_estimators=n_estimators)\n",
    "                    reg = make_pipeline(standard_scaling, model)\n",
    "                    scores = cross_validate(reg, X_train, y_train, scoring=(\"r2\", \"neg_mean_squared_error\"), cv=10, n_jobs=1)  \n",
    "                    mean_r2, std_r2 = scores['test_r2'].mean(), scores['test_r2'].std()\n",
    "                    mean_mse, std_mse = -scores['test_neg_mean_squared_error'].mean(), scores['test_neg_mean_squared_error'].std()\n",
    "                    print(f\"{dataset['name']},{lr},{max_depth},{n_estimators},r2,{mean_r2},{std_r2}\")\n",
    "                    print(f\"{dataset['name']},{lr},{max_depth},{n_estimators},mse,{mean_mse},{std_mse}\")\n",
    "                    f.write(f\"{dataset['name']},{lr},{max_depth},{n_estimators},r2,{mean_r2},{std_r2}\\n\")\n",
    "                    f.write(f\"{dataset['name']},{lr},{max_depth},{n_estimators},mse,{mean_mse},{std_mse}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with medical_charges\n",
      "0.9772675908071864 0.0008128483602677805\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(f\"./datasets/medical_charges\")\n",
    "print(f\"working with medical_charges\")\n",
    "features = list(data.columns)[:-1]\n",
    "labels = list(data.columns)[-1]\n",
    "data_X = data[data.columns.intersection(features)]\n",
    "data_y = data[data.columns.intersection([labels])]\n",
    "X, y = data_X.to_numpy(), data_y.to_numpy().flatten()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2024)\n",
    "\n",
    "# knnt no learned scaling\n",
    "dn_sc = dnnr.DNNR(n_neighbors=20, n_derivative_neighbors=40, scaling='learned', solver=\"scipy_lsqr\", order=\"1\")\n",
    "reg = make_pipeline(standard_scaling, dn_sc)\n",
    "scores = cross_val_score(reg, X_train, y_train, scoring='r2', cv=10, n_jobs=1)\n",
    "print(scores.mean(), scores.std())\n",
    "\n",
    "# # knnt learned scaling\n",
    "# dn_sc = dnnr.DNNR(n_neighbors=5, n_derivative_neighbors=24, scaling=\"learned\", solver=\"scipy_lsqr\", order=\"1\")\n",
    "# reg = make_pipeline(standard_scaling, dn_sc)\n",
    "# scores = cross_val_score(reg, X_train, y_train, scoring='r2', cv=10, n_jobs=1)\n",
    "# print(scores.mean(), scores.std())\n",
    "\n",
    "# # kNN with learned scaling\n",
    "# neigh_r = KNeighborsRegressor(n_neighbors=2)\n",
    "# reg = make_pipeline(standard_scaling, dn_scaling, neigh_r)\n",
    "# scores_knn = cross_val_score(reg, X_train, y_train, scoring='r2', cv=20, n_jobs=2)\n",
    "# print(scores_knn, scores_knn.mean(), scores_knn.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9771315 , 0.97792346, 0.97566964, 0.97716689, 0.97712359,\n",
       "       0.97730052, 0.97609175, 0.97700543, 0.97734594, 0.97719967])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knnt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
